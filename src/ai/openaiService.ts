import OpenAI from "openai";
import { config } from "../config/bot.js";
import { Logger } from "../utils/logger.js";
import type { ProcessedMessage } from "../utils/messageProcessor.js";

export interface AIResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
  finishReason: string;
}

export interface ConversationContext {
  userId: string;
  username: string;
  channelId: string;
  channelName: string;
  guildId?: string;
  guildName?: string;
  recentMessages: Array<{
    role: "user" | "assistant";
    content: string;
    timestamp: number;
    author?: string;
  }>;
  userContext?: string; // From database - what we know about this user
}

export class OpenAIService {
  private client: OpenAI;
  private model: string = "gpt-4.1-mini"; // Cost-effective model for chat
  private maxTokens: number = 500;
  private temperature: number = 0.7;

  constructor() {
    if (!config.openaiApiKey) {
      throw new Error(
        "OpenAI API key not configured. Please set OPENAI_API_KEY in your environment."
      );
    }

    this.client = new OpenAI({
      apiKey: config.openaiApiKey,
    });

    Logger.info("‚úÖ OpenAI service initialized");
  }

  /**
   * Generate AI response based on message and context
   */
  async generateResponse(
    message: ProcessedMessage,
    context: ConversationContext
  ): Promise<AIResponse> {
    try {
      Logger.info(`üß† Generating AI response for ${message.author.username}`);

      const systemPrompt = this.buildSystemPrompt(context);
      const messages = this.buildMessageHistory(message, context);

      const startTime = Date.now();
      const completion = await this.client.chat.completions.create({
        model: this.model,
        messages: [{ role: "system", content: systemPrompt }, ...messages],
        max_tokens: this.maxTokens,
        temperature: this.temperature,
        user: message.author.id, // For abuse monitoring
      });

      const duration = Date.now() - startTime;
      const response = completion.choices[0];

      if (!response?.message?.content) {
        throw new Error("No content in AI response");
      }

      const aiResponse: AIResponse = {
        content: response.message.content,
        usage: completion.usage
          ? {
              promptTokens: completion.usage.prompt_tokens,
              completionTokens: completion.usage.completion_tokens,
              totalTokens: completion.usage.total_tokens,
            }
          : undefined,
        model: completion.model,
        finishReason: response.finish_reason || "unknown",
      };

      Logger.success(
        `‚úÖ AI response generated in ${duration}ms (${aiResponse.usage?.totalTokens} tokens)`
      );
      return aiResponse;
    } catch (error) {
      Logger.error("‚ùå Error generating AI response:", error);
      throw new Error(
        `AI response generation failed: ${
          error instanceof Error ? error.message : "Unknown error"
        }`
      );
    }
  }

  /**
   * Build system prompt based on bot personality and context
   */
  private buildSystemPrompt(context: ConversationContext): string {
    const botPersonality = `B·∫°n l√† m·ªôt AI chatbot trong server Discord "${
      context.guildName || "Server Kh√¥ng X√°c ƒê·ªãnh"
    }". M·ª•c ti√™u ch√≠nh c·ªßa b·∫°n l√† h·ªçc h·ªèi v·ªÅ c√°c th√†nh vi√™n trong server v√† cung c·∫•p nh·ªØng ph·∫£n h·ªìi h·ªØu √≠ch, ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh.

ƒê·∫∑c ƒëi·ªÉm ch√≠nh:
- T√≠nh c√°ch th√¢n thi·ªán v√† h·∫•p d·∫´n
- Nh·ªõ v√† tham kh·∫£o c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ khi ph√π h·ª£p
- ƒê·∫∑t c√¢u h·ªèi ti·∫øp theo ƒë·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ ng∆∞·ªùi d√πng
- Cung c·∫•p th√¥ng tin v√† h·ªó tr·ª£ h·ªØu √≠ch
- Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ng √Ω nghƒ©a (th∆∞·ªùng 1-3 c√¢u)
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√π h·ª£p v·ªõi Discord v√† emoji khi th√≠ch h·ª£p
- T√¥n tr·ªçng v√† bao g·ªìm m·ªçi ng∆∞·ªùi
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch t·ª± nhi√™n v√† th√¢n thi·ªán

Ng·ªØ c·∫£nh hi·ªán t·∫°i:
- Server: ${context.guildName || "Kh√¥ng x√°c ƒë·ªãnh"}
- K√™nh: #${context.channelName}
- Ng∆∞·ªùi d√πng: ${context.username}

${
  context.userContext
    ? `Nh·ªØng g√¨ b·∫°n bi·∫øt v·ªÅ ${context.username}: ${context.userContext}`
    : `ƒê√¢y l√† l·∫ßn t∆∞∆°ng t√°c ƒë·∫ßu ti√™n c·ªßa b·∫°n v·ªõi ${context.username}. H√£y c·ªë g·∫Øng t√¨m hi·ªÉu ƒëi·ªÅu g√¨ ƒë√≥ v·ªÅ h·ªç!`
}

Ghi nh·ªõ: Ph·∫£n h·ªìi c·ªßa b·∫°n s·∫Ω gi√∫p x√¢y d·ª±ng h·ªì s∆° c·ªßa ng∆∞·ªùi d√πng n√†y cho c√°c t∆∞∆°ng t√°c trong t∆∞∆°ng lai. Ch√∫ √Ω ƒë·∫øn s·ªü th√≠ch, t√≠nh c√°ch v√† preferences c·ªßa h·ªç.`;

    return botPersonality;
  }

  /**
   * Build message history for context
   */
  private buildMessageHistory(
    currentMessage: ProcessedMessage,
    context: ConversationContext
  ): Array<{ role: "user" | "assistant"; content: string }> {
    const messages: Array<{ role: "user" | "assistant"; content: string }> = [];

    // Add recent conversation history (limited to last 10 messages to manage token usage)
    const recentMessages = context.recentMessages.slice(-10);

    for (const msg of recentMessages) {
      messages.push({
        role: msg.role,
        content:
          msg.role === "user" ? `${msg.author}: ${msg.content}` : msg.content,
      });
    }

    // Add current message
    messages.push({
      role: "user",
      content: `${currentMessage.author.username}: ${currentMessage.cleanContent}`,
    });

    return messages;
  }

  /**
   * Test the OpenAI connection
   */
  async testConnection(): Promise<boolean> {
    try {
      Logger.info("üß™ Testing OpenAI connection...");

      const completion = await this.client.chat.completions.create({
        model: this.model,
        messages: [
          { role: "system", content: "You are a helpful assistant." },
          {
            role: "user",
            content: 'Say "Hello, I am working!" in exactly those words.',
          },
        ],
        max_tokens: 20,
        temperature: 0,
      });

      const response = completion.choices[0]?.message?.content;
      const isWorking = response?.includes("Hello, I am working!");

      if (isWorking) {
        Logger.success("‚úÖ OpenAI connection test successful");
        return true;
      } else {
        Logger.warn(
          `‚ö†Ô∏è OpenAI connection test unexpected response: ${response}`
        );
        return false;
      }
    } catch (error) {
      Logger.error("‚ùå OpenAI connection test failed:", error);
      return false;
    }
  }

  /**
   * Get current model configuration
   */
  getModelInfo() {
    return {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
    };
  }

  /**
   * Update model configuration
   */
  updateConfig(config: {
    model?: string;
    maxTokens?: number;
    temperature?: number;
  }) {
    if (config.model !== undefined) this.model = config.model;
    if (config.maxTokens !== undefined) this.maxTokens = config.maxTokens;
    if (config.temperature !== undefined) this.temperature = config.temperature;

    Logger.info("üîß OpenAI configuration updated:", this.getModelInfo());
  }
}
